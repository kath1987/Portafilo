# -*- coding: utf-8 -*-
"""PROYECTO_ALGEBRA_MATRICIAL.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1Nlq3b7kB-pM91WxiFa9VgkP8LgB-s4w6

**PROYECTO DE ALGEBRA MATRICIAL**

**INSTALACION DE LAS BIBLOTECAS**
"""

!pip install py3langid

import pandas as pd
import json
from nltk.corpus import PlaintextCorpusReader
import nltk

nltk.download(
    ['all'])

!pip install nltk

"""**LECTURA DE LAS BASES DE DATOS**"""

df1 = pd.read_csv("/content/spam.csv", encoding='latin1')

df1

df2 = pd.read_csv("/content/spam2.csv", encoding='latin1')

df2

df3 = pd.read_csv("/content/spam3.csv", encoding='latin1')

df3

"""**UNION DE LAS BASES DE DATOS**"""

result_df = pd.concat([df1, df2, df3], ignore_index=True)

result_df

"""**BUSQUEDA Y ELEIMINACION DE DATOS NULL**"""

result_df.isnull().sum()

df_sin_nulos = result_df.dropna()

df_sin_nulos

"""**ELIMINACION ACERTADA**"""

df_sin_nulos.isnull().sum()

df_sin_nulos["categoria"].value_counts()

categorias_filtradas = df_sin_nulos[df_sin_nulos["categoria"].isin(["ham", "spam"])]
conteo_categorias = categorias_filtradas["categoria"].value_counts()

print(conteo_categorias)

df_filtrado = df_sin_nulos[df_sin_nulos["categoria"].isin(["ham", "spam"])]

df_filtrado["categoria"].value_counts()

"""**COMPARACION DE LOS DATOS PARA VER SI EXISTE EL SESGO**"""

df_filtrado['categoria'].value_counts().plot.bar()

# Sample 23610 items from the "ham" category
no_spam = df_filtrado[df_filtrado["categoria"] == "ham"].sample(23610)
# Sample 23610 items from the "spam" category with replacement
spam = df_filtrado[df_filtrado["categoria"] == "spam"].sample(23610, replace=True)

df_filtrado = pd.concat([no_spam, spam])
df_filtrado = df_filtrado.sample(frac=1).reset_index(drop=True) #Dessordenar las filas

"""**LIMPIEZA COMPLETADA Y BASE DE DATOS LIMPIA**"""

#Termina Winnifer
df_filtrado

df_filtrado['categoria'].value_counts().plot.bar()

#Eduardo
#Estudio del lenguaje

import py3langid as langid

df_filtrado["Language"] = df_filtrado['coment'].apply(lambda x : langid.classify(x)[0])

dff = df_filtrado [df_filtrado["Language"] == "en"][["coment", "categoria"]]

"""**BASE DE DATOS PRINCIPAL**"""

dff

"""**PROCESAMIENTO DEL TEXTO**"""

#Descargamos las librerias
import nltk
from nltk import PorterStemmer
from nltk.stem.wordnet import WordNetLemmatizer
nltk.download([
    "stopwords", #las stopwords
     "names",    #los nombres
     "vader_lexicon",
     "punkt",
     "wordnet" ])

#Obtener las Stopwords del ingles y los names
stopwords = nltk.corpus.stopwords.words("english")
names = nltk.corpus.names.words()

stopwords

names

"""**OBTENCION DE LOS TOKENS**"""

def get_tokens(series, reduce):
  #reducer es una función que lematiza o deriva el token


    vocabulary = []
    for comment in series:
        for idx, word in enumerate(nltk.word_tokenize(comment)):
            if not word.isalpha(): continue  #las comas, puntos, signos etc
            if word in stopwords: continue
            if word not in names: word = word.lower()
            vocabulary.append(reduce(word))


    return vocabulary

"""**LEMATIZACION**"""

lemmatizer = WordNetLemmatizer()
get_tokens(dff["coment"][:1], lemmatizer.lemmatize)

"""**VOCABULARIO OBTENIDO**"""

#Obtener Vocabulario
vocabulary = get_tokens(dff["coment"][:],lemmatizer.lemmatize )

#Termina Eduardo
vocabulary = list(set(vocabulary))

"""**CODIGO DE APREDIZAJE DIVIDIDO POR SPAM_WORDS Y NO_SPAM_WORDS**"""

#Empieza Katherine
import nltk
from nltk.tokenize import word_tokenize
from nltk.stem import WordNetLemmatizer

lemmatizer = WordNetLemmatizer()

unwanted = set(nltk.corpus.stopwords.words("english"))
unwanted.update(set(w.lower() for w in nltk.corpus.names.words()))

def get_tokens(series, lemmatizer):
    tokens = []
    for text in series:
        words = word_tokenize(text)
        for word in words:
            if word.isalpha() and word.lower() not in unwanted:
                tokens.append(lemmatizer.lemmatize(word))
    return tokens

def skip_unwanted(pos_tuple):
    word, tag = pos_tuple
    if not word.isalpha() or word in unwanted:
        return False
    if tag.startswith("NN"):
        return False
    return True

no_spam = dff[dff["categoria"] == "ham"]["coment"]
spam = dff[dff["categoria"] == "spam"]["coment"]

lemmatizer = WordNetLemmatizer()  # Reemplazo del Lematizer que se estaba usando

no_spam_tokens = get_tokens(no_spam, lemmatizer)
spam_tokens = get_tokens(spam, lemmatizer)

no_spam_words = [word for word, tag in filter(
    skip_unwanted,
    nltk.pos_tag(no_spam_tokens)
)]

spam_words = [word for word, tag in filter(
    skip_unwanted,
    nltk.pos_tag(spam_tokens)
)]

no_spam_words

spam_words

"""**OBTENCION DE LAS 200 PALABRAS MAS COMUNES EN SPAM Y NO_SPAM**"""

from pandas.core import common
spam_fd = nltk.FreqDist(spam_words)
no_spam_fd = nltk.FreqDist(no_spam_words)

common_set = set(spam_fd).intersection(no_spam_fd)

for word in common_set:
    del spam_fd[word]
    del no_spam_fd[word]

top_200_spam = {word for word, count in spam_fd.most_common(200)}
top_200_no_spam = {word for word, count in no_spam_fd.most_common(200)}

import pickle
#Pickle s el proceso de convertir un objeto de Python en un flujo de bytes
#para almacenarlo en un archivo/base de datos
f = open('top_200_spam.pickle', 'wb')
pickle.dump(top_200_spam, f) #Pickle se utliza para almacenar
f.close()

f = open('top_200_no_spam.pickle', 'wb')
pickle.dump(top_200_no_spam, f)
f.close()

top_200_spam

top_200_no_spam

"""**NLTK Pretrained Sentiment Analyzer**"""

from nltk.sentiment.vader import SentimentIntensityAnalyzer
#Es el proceso de determinar 'computacionalmente' si un comentario es spam o no_spam
sia = SentimentIntensityAnalyzer() #Inizializando Sentiment Intensity Analyzer

dff["categoria"].value_counts()

dff[["coment", "categoria"]]

"""**NLTK Naive Bayes Classifier**"""

from statistics import mean

def extract_features(text):

    vocabulary = []
    for idx, word in enumerate(nltk.word_tokenize(text)):
        if not word.isalpha(): continue
        if word in stopwords: continue
        word = word.lower()
        word = lemmatizer.lemmatize(word)
        if word in top_200_spam or top_200_spam:
            vocabulary.append(word)

    fd = nltk.FreqDist(vocabulary)

    return fd

spam_comments = dff[dff["categoria"] == "ham"]["coment"].sample(400)
no_spam_comments = dff[dff["categoria"] == "spam"]["coment"].sample(400)

features = [
    (extract_features(review), "El comentario es spam")
    for review in spam_comments
]
features.extend([
    (extract_features(review), "El comentario no es spam")
    for review in no_spam_comments
])

features

from random import shuffle

train_count = len(features)//2
shuffle(features)
classifier = nltk.NaiveBayesClassifier.train(features[:train_count])#usamos esta funcion para entrenar el aloritmo de NaiveBayes
classifier.show_most_informative_features()
#Un clasificador basado en el algoritmo Naive Bayes. Se utliza para encontrar la probabilidad de una etiqueta en este caso de las palabras

#Comprovando que tan efectivo es

nltk.classify.accuracy(classifier, features[train_count:])

# Prueba con datos que no se han visto

review = "You need to buy this"
classifier.classify(extract_features(review))

"""**Scikit-Learn Naive Bayes Classifier**"""

from sklearn.naive_bayes import (#Aqu1 se importaron los classifier
    BernoulliNB,
    ComplementNB,
    MultinomialNB
)
from sklearn.neighbors import KNeighborsClassifier
from sklearn.tree import DecisionTreeClassifier
from sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier
from sklearn.linear_model import LogisticRegression
from sklearn.neural_network import MLPClassifier
from sklearn.discriminant_analysis import QuadraticDiscriminantAnalysis

classifiers = {
    "BernoulliNB": BernoulliNB(),
    "ComplementNB": ComplementNB(),
    "MultinomialNB": MultinomialNB(),
    "KNeighborsClassifier": KNeighborsClassifier(),
    "DecisionTreeClassifier": DecisionTreeClassifier(),
    "RandomForestClassifier": RandomForestClassifier(),
    "LogisticRegression": LogisticRegression(),
    "MLPClassifier": MLPClassifier(max_iter=30000),
    "AdaBoostClassifier": AdaBoostClassifier(),
}

train_count = len(features) // 4
shuffle(features)

trained_classifiers = {}

for name, sklearn_classifier in classifiers.items():
     classifier = nltk.classify.SklearnClassifier(sklearn_classifier)
     classifier.train(features[:train_count])
     accuracy = nltk.classify.accuracy(classifier, features[train_count:])
     trained_classifiers[name] = classifier
     print(F"{accuracy:.2%} - {name}")

# Dependiendo de su calificacion escojo el que optenga la mas alta

import pickle
f = open('MLPClassifier', 'wb')
pickle.dump(trained_classifiers["MLPClassifier"], f)
f.close()

# Hago una prueva con el casificador con la notas mas alta

f = open('MLPClassifier', 'rb')
deployed_classifier = pickle.load(f)
f.close()

#Estima la probabilidad de que ocurra un evento, como votar o no votar,
#en función de un conjunto de datos determinado de variables independientes.

deployed_classifier.classify(extract_features("you need buy this one"))

"""Operaciones de Álgebra Matricial con la base de datos utilizadas"""

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.decomposition import PCA

# Reemplazar valores NaN (Not a number) con una cadena vacía en la columna 'coment'
result_df['coment'].fillna('', inplace=True)

# Aplicación de TF-IDF (Frecuencia de término x Frecuencia inversa de documento)
tfidf_vectorizer = TfidfVectorizer(max_features=1000)
X_tfidf = tfidf_vectorizer.fit_transform(result_df['coment'])

# Cálculo de la transpuesta de los datos TF-IDF
X_tfidf_transposed = X_tfidf.transpose()

# Reducción de dimensionalidad con PCA (Reduccion de la dimensionalidad)
pca = PCA(n_components=2)
X_tfidf_transposed_reduced = pca.fit_transform(X_tfidf_transposed.toarray())

# Visualización de la matriz TF-IDF Transpuesta
plt.figure(figsize=(5, 5))
plt.scatter(X_tfidf_transposed_reduced[:, 0], X_tfidf_transposed_reduced[:, 1], alpha=0.5)
plt.title('Visualización PCA de la Matriz TF-IDF Transpuesta')
plt.xlabel('Componente Principal 1')
plt.ylabel('Componente Principal 2')
plt.show()

# Obtener las dimensiones de la matriz TF-IDF
n_rows, n_cols = X_tfidf.shape

# Crear una matriz de números aleatorios con las mismas dimensiones
random_matrix = np.random.rand(n_rows, n_cols)

# Convertir la matriz TF-IDF a formato denso (array)
tfidf_array = X_tfidf.toarray()

# Imprimir los datos de las matrices antes de sumar
print("Matriz TF-IDF (primeros 5 elementos):")
print(tfidf_array[:5])  # Imprime los primeros 5 elementos para visualización simplificada
print("\nMatriz Aleatoria (primeros 5 elementos):")
print(random_matrix[:5])  # Imprime los primeros 5 elementos

# Sumar la matriz TF-IDF con la matriz aleatoria
sum_matrix = tfidf_array + random_matrix

# Imprimir el resultado de la suma
print("\nResultado de la Suma (primeros 5 elementos):")
print(sum_matrix[:5])  # Imprime los primeros 5 elementos del resultado

# Reducción de dimensionalidad con PCA
pca_sum = PCA(n_components=2)
sum_matrix_reduced = pca_sum.fit_transform(sum_matrix)

# Visualización de la suma de matrices
plt.figure(figsize=(5, 6))
plt.scatter(sum_matrix_reduced[:, 0], sum_matrix_reduced[:, 1], alpha=0.5)
plt.title('Visualización PCA de la Suma de Matrices TF-IDF')
plt.xlabel('Componente Principal 1')
plt.ylabel('Componente Principal 2')
plt.show()