{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Cf2WZRpE9Zcf",
        "outputId": "b9a93b9b-e2be-47da-ad7d-3921dbb8d279"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting py3langid\n",
            "  Downloading py3langid-0.2.2-py3-none-any.whl (750 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m750.6/750.6 kB\u001b[0m \u001b[31m6.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from py3langid) (1.23.5)\n"
          ]
        }
      ],
      "source": [
        "!pip install py3langid"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import json\n",
        "from nltk.corpus import PlaintextCorpusReader\n",
        "import nltk\n",
        "nltk.download(\n",
        "    ['all'])"
      ],
      "metadata": {
        "id": "5KiJ1YfQITaW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install nltk"
      ],
      "metadata": {
        "id": "Z402Kvx9JPVo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df = pd.read_csv(\"/content/winemag-data-130k-v2.csv\")"
      ],
      "metadata": {
        "id": "J54t6a56JXfT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**PROCESAMIENTO DE LA DATA**"
      ],
      "metadata": {
        "id": "zmEEQQQaVXJt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df"
      ],
      "metadata": {
        "id": "124tcDKBJbO1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df[\"points\"].hist()"
      ],
      "metadata": {
        "id": "gDKAK8hOJjp4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.isnull().sum()"
      ],
      "metadata": {
        "id": "cUwGywa2MUuq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df[\"quality\"] = df[\"points\"].map(lambda x: \"Positive\" if x > 91 else \"Negative\")"
      ],
      "metadata": {
        "id": "diVboYalJpiN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df[\"quality\"].value_counts()"
      ],
      "metadata": {
        "id": "oQyoHyuuOa2U"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df[\"quality\"].value_counts().plot.bar()"
      ],
      "metadata": {
        "id": "W-B43CllKAHA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df[df[\"quality\"] == \"Positive\"].sample(15000)"
      ],
      "metadata": {
        "id": "ewsxUDNsKJ3J"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "Negative = df[df[\"quality\"] == \"Negative\"].sample(15000)\n",
        "Positive = df[df[\"quality\"] == \"Positive\"].sample(15000)"
      ],
      "metadata": {
        "id": "RJ3B_cJ4KXkg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df = pd.concat([Negative, Positive])\n",
        "df = df.sample(frac=1).reset_index(drop=True) #Dessordenar las filas"
      ],
      "metadata": {
        "id": "sd03YEbrKfj3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df['quality'].value_counts().plot.bar()"
      ],
      "metadata": {
        "id": "MHp-jC5-Kgvl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Estudio del lenguaje\n",
        "\n",
        "import py3langid as langid\n",
        "\n",
        "df[\"Language\"] = df['description'].apply(lambda x : langid.classify(x)[0])\n",
        "#función lambda se utliza para crear una nueva columna de marco de datos con etiquetas de idioma"
      ],
      "metadata": {
        "id": "vl56QZCNR4Xh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Nos muestra las descripciones que no estan en english\n",
        "df[df[\"Language\"] != \"en\"][[\"Language\", \"description\"]]"
      ],
      "metadata": {
        "id": "9jeVm9ZQS1hV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dff = df [df[\"Language\"] == \"en\"][[\"description\", \"quality\"]]"
      ],
      "metadata": {
        "id": "Uvq_7N71zm__"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dff"
      ],
      "metadata": {
        "id": "Sim_sxAEUHSi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**PROCESAMIENTO DEL TEXTO**"
      ],
      "metadata": {
        "id": "WOhR4pyfVSYi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Descargamos las librerias\n",
        "import nltk\n",
        "from nltk import PorterStemmer\n",
        "from nltk.stem.wordnet import WordNetLemmatizer\n",
        "nltk.download([\n",
        "    \"stopwords\", #las stopwords\n",
        "     \"names\",    #los nombres\n",
        "     \"vader_lexicon\",\n",
        "     \"punkt\",\n",
        "     \"wordnet\" ])"
      ],
      "metadata": {
        "id": "8Z3GsHuwV0zk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Obtener las Stopwords del ingles y los names\n",
        "stopwords = nltk.corpus.stopwords.words(\"english\")\n",
        "names = nltk.corpus.names.words()"
      ],
      "metadata": {
        "id": "BrxVFrzkXdcK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "stopwords"
      ],
      "metadata": {
        "id": "35udvK-PV714"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "names"
      ],
      "metadata": {
        "id": "zkVuJHgYWGk6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "def get_tokens(series, reduce):\n",
        "  #reducer es una función que lematiza o deriva el token\n",
        "\n",
        "\n",
        "    vocabulary = []\n",
        "    for comment in series:\n",
        "        for idx, word in enumerate(nltk.word_tokenize(comment)):\n",
        "            if not word.isalpha(): continue  #las comas, puntos, signos etc\n",
        "            if word in stopwords: continue\n",
        "            if word not in names: word = word.lower()\n",
        "            vocabulary.append(reduce(word))\n",
        "\n",
        "\n",
        "    return vocabulary"
      ],
      "metadata": {
        "id": "wNozYasMZ5Es"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "lemmatizer = WordNetLemmatizer()\n",
        "get_tokens(dff[\"description\"][:1], lemmatizer.lemmatize)"
      ],
      "metadata": {
        "id": "232J4NA_eWya"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Obtener Vocabulario\n",
        "vocabulary = get_tokens(dff[\"description\"][:],lemmatizer.lemmatize )"
      ],
      "metadata": {
        "id": "PThsh9Yzfkgp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "vocabulary = list(set(vocabulary))"
      ],
      "metadata": {
        "id": "iw3iL3tSjDAa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Obtenemos los onigramas\n",
        "fd = nltk.FreqDist(vocabulary)\n",
        "fd.tabulate(10)"
      ],
      "metadata": {
        "id": "-2TqobUzgkDZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Obtenemos los bigramas\n",
        "\n",
        "finder = nltk.collocations.BigramCollocationFinder.from_words(vocabulary)\n",
        "finder.ngram_fd.tabulate(10)"
      ],
      "metadata": {
        "id": "mQJrwkzt8VdW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#obtenemos los trigramas\n",
        "\n",
        "finder = nltk.collocations.TrigramCollocationFinder.from_words(vocabulary)\n",
        "finder.ngram_fd.tabulate(10)"
      ],
      "metadata": {
        "id": "b9SX2QY_8aTG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#obtenemos los quadigramas\n",
        "\n",
        "finder = nltk.collocations.QuadgramCollocationFinder.from_words(vocabulary)\n",
        "finder.ngram_fd.tabulate(10)"
      ],
      "metadata": {
        "id": "0FsUyezf8c8y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Aqui buscamos las caracteristicas deseas y las no deseadas\n",
        "\n",
        "unwanted = nltk.corpus.stopwords.words(\"english\")\n",
        "unwanted.extend([w.lower() for w in nltk.corpus.names.words()]) #El método w.lower devuelve una cadena donde todos los caracteres están en minúsculas.\n",
        "\n",
        "def skip_unwanted(pos_tuple):\n",
        "    word, tag = pos_tuple\n",
        "    if not word.isalpha() or word in unwanted:\n",
        "        return False\n",
        "    if tag.startswith(\"NN\"):\n",
        "        return False\n",
        "    return True\n",
        "\n",
        "positive_description = dff[dff[\"quality\"] == \"Positive\"][\"description\"]\n",
        "negative_description = dff[dff[\"quality\"] == \"Negative\"][\"description\"]\n",
        "\n",
        "positive_tokens = get_tokens(positive_description,lemmatizer.lemmatize)\n",
        "negative_tokens = get_tokens(negative_description,lemmatizer.lemmatize)\n",
        "\n",
        "positive_words = [word for word, tag in filter(\n",
        "    skip_unwanted,\n",
        "    nltk.pos_tag(positive_tokens)\n",
        ")]\n",
        "negative_words = [word for word, tag in filter(\n",
        "    skip_unwanted,\n",
        "    nltk.pos_tag(negative_tokens)\n",
        ")]"
      ],
      "metadata": {
        "id": "aUBkmADu51pn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "positive_words"
      ],
      "metadata": {
        "id": "4_mPRNDE59UW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "negative_words"
      ],
      "metadata": {
        "id": "dooJWlhv6Brt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from pandas.core import common\n",
        "positive_fd = nltk.FreqDist(positive_words)\n",
        "negative_fd = nltk.FreqDist(negative_words)\n",
        "\n",
        "common_set = set(positive_fd).intersection(negative_fd)\n",
        "\n",
        "for word in common_set:\n",
        "    del positive_fd[word]\n",
        "    del negative_fd[word]\n",
        "\n",
        "top_200_positive = {word for word, count in positive_fd.most_common(200)}\n",
        "top_200_negative = {word for word, count in negative_fd.most_common(200)}"
      ],
      "metadata": {
        "id": "jKpba0WzC9DO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pickle\n",
        "#Pickle s el proceso de convertir un objeto de Python en un flujo de bytes\n",
        "#para almacenarlo en un archivo/base de datos\n",
        "f = open('top_200_positive.pickle', 'wb')\n",
        "pickle.dump(top_200_positive, f) #Pickle se utliza para almacenar\n",
        "f.close()\n",
        "\n",
        "f = open('top_200_negative.pickle', 'wb')\n",
        "pickle.dump(top_200_negative, f)\n",
        "f.close()"
      ],
      "metadata": {
        "id": "8m4ENeLkDAem"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "top_200_positive"
      ],
      "metadata": {
        "id": "DpbtZ3AKDBlk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "top_200_negative"
      ],
      "metadata": {
        "id": "H6GUOgp7DJkj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**NLTK Pretrained Sentiment Analyzer**"
      ],
      "metadata": {
        "id": "UN48e-LmDZ1D"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
        "#Es el proceso de determinar 'computacionalmente' si un comentario es positivo o negativo\n",
        "sia = SentimentIntensityAnalyzer() #Inizializando Sentiment Intensity Analyzer"
      ],
      "metadata": {
        "id": "E5gBLPxTDdRW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dff[\"quality\"].value_counts()"
      ],
      "metadata": {
        "id": "dFOzdh4ADgmH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dff[\"quality_Estimate\"] = dff[\"description\"].map(lambda x : max(sia.polarity_scores(x), key=sia.polarity_scores(x).get))"
      ],
      "metadata": {
        "id": "cn23vMsoDl30"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dff[[\"description\", \"quality\", \"quality_Estimate\"]]"
      ],
      "metadata": {
        "id": "U1160HHTES44"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**NLTK Naive Bayes Classifier**"
      ],
      "metadata": {
        "id": "XDi08oJzElX8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from statistics import mean\n",
        "\n",
        "def extract_features(text):\n",
        "\n",
        "    vocabulary = []\n",
        "    for idx, word in enumerate(nltk.word_tokenize(text)):\n",
        "        if not word.isalpha(): continue\n",
        "        if word in stopwords: continue\n",
        "        word = word.lower()\n",
        "        word = lemmatizer.lemmatize(word)\n",
        "        if word in top_200_negative or top_200_positive:\n",
        "            vocabulary.append(word)\n",
        "\n",
        "    fd = nltk.FreqDist(vocabulary)\n",
        "\n",
        "    return fd"
      ],
      "metadata": {
        "id": "u8LYjEC4HSnc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "positive_comments = dff[dff[\"quality\"] == \"Positive\"][\"description\"].sample(200)\n",
        "negative_comments = dff[dff[\"quality\"] == \"Negative\"][\"description\"].sample(200)\n",
        "\n",
        "features = [\n",
        "    (extract_features(review), \"La review es positiva\")\n",
        "    for review in positive_comments\n",
        "]\n",
        "features.extend([\n",
        "    (extract_features(review), \"La review es negativa\")\n",
        "    for review in negative_comments\n",
        "])"
      ],
      "metadata": {
        "id": "D3W6w5W8P_d9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "features"
      ],
      "metadata": {
        "id": "IRBJF4aSPzwc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from random import shuffle\n",
        "\n",
        "train_count = len(features)//2\n",
        "shuffle(features)\n",
        "classifier = nltk.NaiveBayesClassifier.train(features[:train_count])#usamos esta funcion para entrenar el aloritmo de NaiveBayes\n",
        "classifier.show_most_informative_features()\n",
        "#Un clasificador basado en el algoritmo Naive Bayes. Se utliza para encontrar la probabilidad de una etiqueta en este caso de las palabras"
      ],
      "metadata": {
        "id": "d7RkeBDrQeOw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Comprovando que tan efectivo es\n",
        "\n",
        "nltk.classify.accuracy(classifier, features[train_count:])"
      ],
      "metadata": {
        "id": "RNKAo2AED0Lr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Prueba con datos que no se han visto\n",
        "\n",
        "review = \"i love it\"\n",
        "classifier.classify(extract_features(review))"
      ],
      "metadata": {
        "id": "of90GGuyD1ud"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Scikit-Learn Naive Bayes Classifier**"
      ],
      "metadata": {
        "id": "NNH_F9AjFqm6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.naive_bayes import (#Aqu1 se importaron los classifier\n",
        "    BernoulliNB,\n",
        "    ComplementNB,\n",
        "    MultinomialNB\n",
        ")\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.neural_network import MLPClassifier\n",
        "from sklearn.discriminant_analysis import QuadraticDiscriminantAnalysis"
      ],
      "metadata": {
        "id": "775C8DXNREgF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "classifiers = {\n",
        "    \"BernoulliNB\": BernoulliNB(),\n",
        "    \"ComplementNB\": ComplementNB(),\n",
        "    \"MultinomialNB\": MultinomialNB(),\n",
        "    \"KNeighborsClassifier\": KNeighborsClassifier(),\n",
        "    \"DecisionTreeClassifier\": DecisionTreeClassifier(),\n",
        "    \"RandomForestClassifier\": RandomForestClassifier(),\n",
        "    \"LogisticRegression\": LogisticRegression(),\n",
        "    \"MLPClassifier\": MLPClassifier(max_iter=30000),\n",
        "    \"AdaBoostClassifier\": AdaBoostClassifier(),\n",
        "}"
      ],
      "metadata": {
        "id": "wFtOaI-dR4K0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_count = len(features) // 4\n",
        "shuffle(features)\n",
        "\n",
        "trained_classifiers = {}\n",
        "\n",
        "for name, sklearn_classifier in classifiers.items():\n",
        "     classifier = nltk.classify.SklearnClassifier(sklearn_classifier)\n",
        "     classifier.train(features[:train_count])\n",
        "     accuracy = nltk.classify.accuracy(classifier, features[train_count:])\n",
        "     trained_classifiers[name] = classifier\n",
        "     print(F\"{accuracy:.2%} - {name}\")"
      ],
      "metadata": {
        "id": "vEUPfTKXTBt-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Dependiendo de su calificacion escojo el que optenga la mas alta\n",
        "\n",
        "import pickle\n",
        "f = open('quality_classifier.pickle', 'wb')\n",
        "pickle.dump(trained_classifiers[\"LogisticRegression\"], f)\n",
        "f.close()"
      ],
      "metadata": {
        "id": "BYDuYUvoytWw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Hago una prueva con el casificador con la notas mas alta\n",
        "\n",
        "f = open('quality_classifier.pickle', 'rb')\n",
        "deployed_classifier = pickle.load(f)\n",
        "f.close()\n",
        "\n",
        "#Estima la probabilidad de que ocurra un evento, como votar o no votar,\n",
        "#en función de un conjunto de datos determinado de variables independientes."
      ],
      "metadata": {
        "id": "xvCLQdtYyuIn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "deployed_classifier.classify(extract_features(\"it was so hoorrible i did not like it\"))"
      ],
      "metadata": {
        "id": "Q5Oke2oYy1EC"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}